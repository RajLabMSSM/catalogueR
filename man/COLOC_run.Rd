% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/COLOC_run.R
\name{COLOC_run}
\alias{COLOC_run}
\title{Iteratively run coloc on merged GWAS-QTL datatables}
\usage{
COLOC_run(
  gwas.qtl_paths,
  save_path = tempfile(pattern = "coloc_results", fileext = ".tsv.gz"),
  top_snp_only = TRUE,
  split_by_group = FALSE,
  method = "abf",
  coloc_thresh = 0.8,
  compute_n = NULL,
  nThread = 1,
  verbose = TRUE
)
}
\arguments{
\item{gwas.qtl_paths}{Query results paths from 
\link[catalogueR]{eQTLcatalogue_query}.}

\item{save_path}{Where to save results to.}

\item{top_snp_only}{Only include the SNP (with the highest SNP-wise PP.H4, 
which is usually the one with the smallest p-value) instead of all SNPs. 
Can be useful for reducing data size.}

\item{split_by_group}{Split files by QTL group when saving.}

\item{method}{Method for querying eQTL Catalogue:
\itemize{
\item{"REST" (default): }{Uses the REST API. Slow but can be used by anyone.}
\item{"tabix"}{Uses tabix \link[echotabix]{query}.
Fast, but requires the user to first get their IP address whitelisted 
by the EMBL-EBI server admin by putting in a request
\href{https://www.ebi.ac.uk/about/contact/support/}{here}.} 
}
\emph{Note}: "tabix" is about ~17x faster than the REST API,
 but is currently a far less reliable method than the REST API because 
 tabix tends to get blocked by eQTL Catalogue's firewall.
See \href{https://github.com/RajLabMSSM/catalogueR/issues/5}{here}
for more details.}

\item{coloc_thresh}{Colocalization Posterior Probability threshold,
 using the formula:
\code{(PP.H3 + PP.H4 >= coloc_thresh) & (PP.H4 / PP.H3 >= 2}.}

\item{compute_n}{How to compute per-SNP sample size (new column "N").
\itemize{
\item{\code{0}: }{N will not be computed.}
\item{\code{>0}: }{If any number >0 is provided,
that value will be set as N for every row.
\strong{Note}: Computing N this way is incorrect and should be avoided
if at all possible.}
\item{\code{"sum"}: }{N will be computed as:
cases (N_CAS) + controls (N_CON), so long as both columns are present}.
\item{\code{"ldsc"}: }{N will be computed as effective sample size:
Neff =(N_CAS+N_CON)*(N_CAS/(N_CAS+N_CON)) / mean((N_CAS/(N_CAS+N_CON))(N_CAS+N_CON)==max(N_CAS+N_CON))}.
\item{\code{"giant"}: }{N will be computed as effective sample size:
Neff = 2 / (1/N_CAS + 1/N_CON)}.
\item{\code{"metal"}: }{N will be computed as effective sample size:
Neff = 4 / (1/N_CAS + 1/N_CON)}.
}}

\item{nThread}{The number of CPU cores you want to use to speed up your
queries through parallelization.}

\item{verbose}{Print messages.}
}
\value{
If \code{top_snp_only=TRUE}, returns SNP-level stats for only the SNP
with the highest colocalization probability (\emph{SNP.PP.H4})
If \code{top_snp_only=FALSE}, returns SNP-level stats for every SNP.
In either case, summary-level coloc stats are added in the columns
\emph{PP.H0}, \emph{PP.H1}, \emph{PP.H2}, \emph{PP.H3}, \emph{PP.H4}.
}
\description{
Runs colocalization tests (\link[coloc]{coloc.abf})
on merged GWAS-QTL \emph{data.tables}
generated by \link[catalogueR]{eQTLcatalogue_query}.
Iteratively runs coloc across each:
\itemize{
\item{QTL dataset}
\item{GWAS locus}
\item{QTL gene}
}
\emph{NOTE:} Assumes that each file is within a subfolder named after
 the QTL dataset it came from.
}
\examples{
gwas.qtl_paths <- catalogueR::eQTLcatalogue_example_queries()
coloc_QTLs <- catalogueR::COLOC_run(gwas.qtl_paths = gwas.qtl_paths)
}
\seealso{
Other coloc: 
\code{\link{COLOC_corplot}()},
\code{\link{COLOC_get_example_res}()},
\code{\link{COLOC_get_res}()},
\code{\link{COLOC_heatmap}()},
\code{\link{COLOC_merge_res}()},
\code{\link{COLOC_report_summary}()}
}
\concept{coloc}
