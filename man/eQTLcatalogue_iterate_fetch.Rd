% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/eQTLcatalogue_iterate_fetch.R
\name{eQTLcatalogue_iterate_fetch}
\alias{eQTLcatalogue_iterate_fetch}
\title{Iterate queries to \emph{eQTL Catalogue}}
\source{
\code{
sumstats_paths <- echodata::get_Nalls2019_loci(limit_snps = 5)
qtl_id <- catalogueR::eQTLcatalogue_list_datasets()$unique_id[1]
GWAS.QTL <- catalogueR:::eQTLcatalogue_iterate_fetch(
    sumstats_paths = sumstats_paths,
     qtl_id = qtl_id,
     nThread = 1,
     split_files = FALSE)
}
}
\usage{
eQTLcatalogue_iterate_fetch(
  sumstats_paths,
  output_dir = file.path(tempdir(), "catalogueR_queries"),
  qtl_id,
  method = c("REST", "tabix"),
  quant_method = "ge",
  multithread_loci = TRUE,
  multithread_tabix = FALSE,
  split_files = TRUE,
  merge_with_gwas = FALSE,
  force_new_subset = FALSE,
  query_genome = "hg19",
  conda_env = "echoR_mini",
  nThread = 1,
  verbose = TRUE
)
}
\arguments{
\item{sumstats_paths}{A list of paths to any number of summary stats files
whose coordinates you want to use to make queries to eQTL Catalogue.
If you wish to add custom names to the loci, simply add these as the
names of the path list
 (e.g. \code{c(BST1="<path>/<to>/<BST1_file>",
  LRRK2="<path>/<to>/<LRRK2_file>")}).
 Otherwise, loci will automatically named based on their min/max
 genomic coordinates.

The minimum columns in these files required to make queries include:
\describe{
\item{SNP}{RSID of each SNP.}
\item{CHR}{Chromosome (can be in "chr12" or "12" format).}
\item{POS}{Genomic position of each SNP.}
\item{...}{Optional extra columns.}
}}

\item{output_dir}{The folder you want the merged gwas/qtl results to be
saved to (set to \code{NULL} to not save the results).
If \code{split_files=FALSE}, all query results will be merged into one and
saved as \emph{<output_dir>/eQTLcatalogue_tsv.gz}.
If \code{split_files=TRUE}, all query results will instead be split into
 smaller files and stored in \emph{<output_dir>/}.}

\item{method}{Method for querying eQTL Catalogue:
\itemize{
\item{"REST" (default): }{Uses the REST API. Slow but can be used by anyone.}
\item{"tabix"}{Uses tabix \link[echotabix]{query}.
Fast, but requires the user to first get their IP address whitelisted 
by the EMBL-EBI server admin by putting in a request
\href{https://www.ebi.ac.uk/about/contact/support/}{here}.} 
}
\emph{Note}: "tabix" is about ~17x faster than the REST API,
 but is currently a far less reliable method than the REST API because 
 tabix tends to get blocked by eQTL Catalogue's firewall.
See \href{https://github.com/RajLabMSSM/catalogueR/issues/5}{here}
for more details.}

\item{quant_method}{eQTL Catalogue actually contains more than just
 eQTL data.
For each dataset, the following kinds of QTLs can be queried:
\describe{
\item{gene expression QTL}{\code{quant_method="ge"} (\emph{default})
or \code{quant_method="microarray"}, depending on the dataset.
\strong{catalogueR} will automatically select whichever option is available.}
\item{exon expression QTL}{\emph{*under construction*}
\code{quant_method="ex"}}
\item{transcript usage QTL}{\emph{*under construction*}
\code{quant_method="tx"}}
\item{promoter, splice junction and 3' end usage QTL}{
\emph{*under construction*}  \code{quant_method="txrev"}}
}}

\item{multithread_tabix}{Multi-thread across within a single tabix file query
(good when you have one-several large loci).}

\item{split_files}{Save the results as one file per QTL dataset
(with all loci within each file).
If this is set to \code{=TRUE}, then this function will return the list of
paths where these files were saved.
A helper function is provided to import and merge them back together in R.
If this is set to  \code{=FALSE}, then this function will instead return one
big merged \link[data.table]{data.table}
containing results from all QTL datasets and all loci.
\code{=FALSE} is not recommended when you have many large loci and/or many
QTL datasets,
because you can only fit so much data into memory.}

\item{merge_with_gwas}{Whether you want to merge your QTL query results
with your GWAS data
(convenient, but takes up more storage).}

\item{force_new_subset}{By default, \strong{catalogueR} will use any
pre-existing files that match your query.
Set \code{force_new_subset=T} to override this and force a new query.}

\item{query_genome}{The genome build of your query coordinates
(e.g. \code{query_dat}).
If your coordinates are in \emph{hg19}, \strong{catalogueR} will
automatically lift them over
to \emph{hg38} (as this is the build that eQTL Catalogue uses).}

\item{conda_env}{Conda environment to search for tabix executable in.}

\item{nThread}{The number of CPU cores you want to use to speed up your
queries through parallelization.}

\item{verbose}{Show more (\code{=TRUE}) or fewer (\code{=FALSE}) messages.}
}
\description{
Uses coordinates from stored summary stats files (e.g. GWAS)
to determine which regions to query from \emph{eQTL Catalogue}.
}
\seealso{
Other eQTL Catalogue: 
\code{\link{eQTLcatalogue_fetch}()},
\code{\link{eQTLcatalogue_header}},
\code{\link{eQTLcatalogue_query}()},
\code{\link{eQTLcatalogue_search_metadata}()},
\code{\link{fetch_restAPI}()},
\code{\link{fetch_tabix}()},
\code{\link{merge_gwas_qtl}()},
\code{\link{meta}}
}
\concept{eQTL Catalogue}
\keyword{internal}
